{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNPgDfWWj/ffkh8Kgx4yFdU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JulianaCarvajal/Spaceship_Titanic/blob/workOnPreprocess%2FBustamJos3/classConstructionPlayground.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Working from remote\n",
        "Upload .py file and import class from it"
      ],
      "metadata": {
        "id": "q1V72fCt3lvk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ewp0J9GBIecu"
      },
      "outputs": [],
      "source": [
        "class experimentalIterator: # class construction\n",
        "    import pandas as pd #libraries\n",
        "\n",
        "\n",
        "    from sklearn.model_selection import train_test_split #BustamJos3 for models # import train_test split function\n",
        "    from sklearn.model_selection import GridSearchCV # hyperparameter getter\n",
        "    from sklearn.ensemble import RandomForestClassifier #import RandomForest\n",
        "    import seaborn as sns #visualization\n",
        "    import matplotlib.pyplot as plt\n",
        "    %matplotlib inline\n",
        "\n",
        "    def __init__(self, X, y):\n",
        "        self._X = self.dtypeConvertion( X )#store as attribute X data of dataset\n",
        "        self._y = self.dtypeConvertion( y )#same as above but y\n",
        "    \n",
        "    def dtypeConvertion(self, df): #convert dtypes to best fit #previous to BustamJos3 preprocess\n",
        "        return df.convert_dtypes(infer_objects=True)\n",
        "    \n",
        "    def dropNotRelevant(self,listToDrop): # start of BustamJos3 preprocess\n",
        "        self._X.drop(listToDrop, axis=1, inplace=True)\n",
        "    \n",
        "    def preProcess(self):\n",
        "        listCategoric=[str(i) for i in (self._X.dtypes=='string').loc[(self._X.dtypes=='string')==True].index] # take name cols which are categorical\n",
        "        XCategoric=self._X[listCategoric].fillna('wanted').values #to handle oneHotEncoder, replace NaN values with 'wanted'\n",
        "        listNumeric=[str(i) for i in (self._X.dtypes=='Int64').loc[(self._X.dtypes=='Int64')==True].index] #numeric col names\n",
        "        XNumeric=self._X[listNumeric].fillna(-1).values # take the numeric types only\n",
        "        # Transformation on categorical cols with labelEncoder and NaN Imputation categorical\n",
        "        from sklearn.preprocessing import LabelEncoder #BustamJos3 preprocess  #convert categorical to numerical\n",
        "        from sklearn.impute import KNNImputer # nan imputation\n",
        "        labelEncoder=LabelEncoder() #instancing\n",
        "        dict_to_replace={}\n",
        "        for i in range(XCategoric.shape[1]): # labelEncoding and replacing consecutively\n",
        "            array_to_replace=labelEncoder.fit_transform( XCategoric[:,i] ).reshape(-1,1) #labelEncoding\n",
        "            kNNImputer=KNNImputer(n_neighbors=1, missing_values=len(labelEncoder.classes_)-1,weights='distance') #imputation for categoric\n",
        "            array_to_replace=kNNImputer.fit_transform( array_to_replace ) #imputation\n",
        "            dict_to_replace[i]=array_to_replace\n",
        "        import numpy as np # to concatenation\n",
        "        XCategoric=np.concatenate( (dict_to_replace[0], dict_to_replace[1], dict_to_replace[2]), axis=1 ) #concatenate\n",
        "        from sklearn.preprocessing import OneHotEncoder #oneHotEncoding for categoric cols\n",
        "        oneHotEncoder=OneHotEncoder(handle_unknown='error',sparse=False) # instancing\n",
        "        convertedOHE=oneHotEncoder.fit_transform(XCategoric[:,[0,2]]) # tranformation to get convertion to OHE  #array with categories of n-categoric cols\n",
        "        # .reshape on XCategoric for dimensionality  and concatenate\n",
        "        convertedOHE= np.concatenate( (convertedOHE[:,:len(oneHotEncoder.categories_[0])-1], XCategoric[:,1].reshape((-1,1)), convertedOHE[:,len(oneHotEncoder.categories_[0])-1:7 ]), axis=1 )\n",
        "        # nanImputation for numeric cols\n",
        "        kNNImputer=KNNImputer(n_neighbors=1, missing_values=-1,weights='distance')\n",
        "        imputedNumeric=kNNImputer.fit_transform(XNumeric)\n",
        "        from sklearn.preprocessing import StandardScaler # standardization\n",
        "        imputedNumeric=StandardScaler().fit_transform(imputedNumeric) # standardization\n",
        "        self._X=np.concatenate( (imputedNumeric, convertedOHE), axis=1 )\n",
        "        #TODO\n",
        "        #1. evaluate standardization only on numeric cols and then concantenate\n",
        "        #2. make implementation of selected models according to discussed"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# End of working from remote"
      ],
      "metadata": {
        "id": "6hJW_D-p3o51"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Working from local"
      ],
      "metadata": {
        "id": "OGb867H73rRF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cK_pPA3nJerC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# End of working from local"
      ],
      "metadata": {
        "id": "y7acWBvC3tEl"
      }
    }
  ]
}